***AUTHOR OF THIS SUMMARY:* SHATADRU MAJUMDAR**

***LINKEDIN PROFILE:***
https://www.linkedin.com/in/shatadru-majumdar-ab262317b

***FaceForensics++: Learning to Detect Manipulated Facial Images***

Andreas R¨ossler^1^, Davide Cozzolino^2^, Luisa Verdoliva^2^, Christian
Riess^3^, Justus Thies^1^ Matthias Nießner^1^

^1^Technical University of Munich, ^2^University Federico II of Naples,
^3^University of Erlangen-Nuremberg

***IN A GLANCE***

The rapid progress in synthetic face generation and manipulation poses
significant threat, like, loss of trust in online content, spreading of
false news and many more. This paper gives an overview of the current
state-of-the-art face manipulation methods, the existing Forensic
analysis datasets, the different forgery detector architectures.

This paper makes the following contributions:

-   an automated benchmark for facial manipulation detection under
    random compression for a standardized comparison, including a human
    baseline

-   a novel large-scale dataset of manipulated facial imagery composed
    of more than 1.8 million images from 1,000 videos with pristine
    (i.e., real) sources and target ground truth to enable supervised
    learning

-   an extensive evaluation of state-of-the-art hand-crafted and learned
    forgery detectors in various scenarios

-   a state-of-the-art forgery detection method tailored to facial
    manipulations.

***PROPOSED DATASET***

-   They created the *FaceForensics++* dataset, which is an extension
    from the previous FaceForensics dataset and is a large-scale dataset
    which enables researchers to train state-of-the-art forgery
    detectors for facial image manipulation in a supervised fashion.

-   They downloaded 1000 pristine videos from the Internet, performed
    manual screening of the resulting clips to ensure a high-quality
    video selection and to avoid videos with face occlusions. They
    selected 1,000 video sequences containing 509,914 images which to
    use as their pristine data.

-   To generate a large scale manipulation dataset, they adopted two
    computer-graphics based approaches (*Face2Face* and *FaceSwap*) and
    two deep-learning based approaches (*DeepFakes* and
    *NeuralTextures*).

<!-- -->

-   ***Face2Face:*** *Face2Face* is a facial reenactment system that
    transfers the expressions of a source video to a target video while
    maintaining the identity of the target person.

-   ***FaceSwap:*** *FaceSwap* is a graphics-based approach to transfer
    the face region from a source video to a target video.

-   ***DeepFakes:*** In *DeepFakes*, a face in a target sequence is
    replaced by a face that has been observed in a source video or image
    collection.

-   ***NeuralTextures:*** *NeuralTextures* uses the original video data
    to learn a neural texture of the target person, including a
    rendering network. This is trained with a photometric reconstruction
    loss in combination with an adversarial loss.

<!-- -->

-   To create a realistic setting for manipulated videos, they generate
    output videos with different quality levels – a) Raw

    b\) High Quality (constant rate quantization parameter 23)

    c\) Low Quality (constant rate quantization parameter 40)

    ![Capture.JPG](media/image1.jpeg){width="4.2in" height="2.65in"}

***BENCHMARK ARCHITECTURES ON FACEFORENSICS++ DATASET***

-   ***Human Observers:*** They conducted a user-study with 204
    participants, gave them a 2, 4, 6 second time-limit to view an image
    and then asked them to classify the image as Real or Fake. Each
    participant classified 60 images.

-   Automatic Forgery Detection Methods: They used additional
    domain-specific information to improve accuracy. First, they tracked
    the face in the image, cropped it out and they enlarged it by a
    factor of 1.3 and this region is input into a learned classification
    network to output prediction.

> ![Automatic
> Forgery.JPG](C:\Users\Shatadru Majumdar\Desktop\Papers\Capture.JPG){width="4.613636264216973in"
> height="1.0499015748031495in"}
>
> They applied the following classification netwiorks:

-   Detection based on Steganalysis Features: This employs hand-crafted
    features which are co-occurrences based on 4 pixels pattern. The
    total feature length is 162 and is then used to train a Support
    Vector Machine classifier.

-   Detection based on Learned Features:

1.  Cozzolino et al. \[1\] cast the hand-crafted Steganalysis features
    to a CNN based network and they trained this on their dataset.

2.  They used their dataset to train the convolutional neural network
    architecture proposed by Bayar and Stamm \[2\] that uses a
    constrained convolutional layer followed by two convolutional, two
    max-pooling and three fully connected layers.

3.  They trained the CNN architecture with global pooling layer that
    computes four statistics (mean, variance, maximum and minimum) which
    was proposed by Rahmouni et al. \[3\].

4.  *MesoInception-4* \[4\] is a CNN-based network inspired by
    InceptionNet. The network has two inception modules and two classic
    convolution layers interlaced with max-pooling layers. Afterwards,
    there are 2 fully connected layers. Instead of cross-entropy loss,
    the authors of MesoInception-4 used mean-squared error.

5.  *XceptionNet* \[5\] is a traditional CNN trained on ImageNet based
    on separable convolutions with residual connections. The authors
    transferred it to their task by replacing the final fully connected
    layer with two outputs. The other layers were initialized with the
    ImageNet weights.

-   XceptionNet Full Image: The authors also trained the XceptionNet
    without domain specific information (that is, using the full image
    instead of cropping the face) to see the effect on the accuracy.

-   XceptionNet on Varying corpus: The authors also trained the
    XceptionNet on difeerent corpus size to show the importance of
    corpus sizes.

***RESULTS***

![](media/image3.jpeg){width="3.3506944444444446in"
height="3.1125in"}![2.JPG](media/image4.jpeg){width="3.9583333333333335in"
height="3.125in"}
![3.JPG](media/image5.jpeg){width="3.0911581364829397in"
height="2.866071741032371in"}
![5.JPG](media/image6.jpeg){width="3.408333333333333in"
height="2.5541666666666667in"}
![4.JPG](media/image7.jpeg){width="3.3873108048993874in"
height="3.279174321959755in"}

***CONCLUSION OF AUTHORS***

-   All approaches achieve very high performance on raw input data.
    Performance drops for compressed videos, particularly for
    hand-crafted features and for shallow CNN architectures. The neural
    networks are better at handling these situations. The automated
    detectors outperform human observations by a large margin.

-   XceptionNet is able to achieve compelling results on weak
    compression while still maintaining reasonable performance on low
    quality images.

-   The XceptionNet trained on full image instead of just facial part
    had a significantly lower accuracy as it suffers from lack of domain
    specific knowledge.

-   The XceptionNet classifier trained with varying corpus sizes showed
    that performance increases with increase in corpus size which is
    specifically important for Low Quality images.

***FINAL CONCLUSION:***

To summarize, the XceptionNet performs best among the automated
detectors and it’s accuracy is maximum in the presence of domain
specific knowledge (that is, when facial part is cropped and enlarged by
a factor of 1.3) and large corpus size (specifically for Low Quality
images).

***REFERENCES***

1.  Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. Recasting
    residual-based local descriptors as convolutional neural networks:
    an application to image forgery detection. In *ACM Workshop on
    Information Hiding and Multimedia Security*, pages 1–6, 2017.

2.  Belhassen Bayar and Matthew C. Stamm. A deep learning approach to
    universal image manipulation detection using a new convolutional
    layer. In *ACM Workshop on Information Hiding and Multimedia
    Security,* pages 5–10, 2016.

3.  Nicolas Rahmouni, Vincent Nozick, Junichi Yamagishi, and Isao
    Echizen. Distinguishing computer graphics from natural images using
    convolution neural networks. In *IEEE Workshop on Information
    Forensics and Security,* pages 1–6, 2017.

4.  Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen.
    Mesonet: a compact facial video forgery detection network. *arXiv
    preprint arXiv:1809.00888*, 2018.

5.  Francois Chollet. Xception: Deep Learning with Depthwise Separable
    Convolutions. In *IEEE Conference on Computer Vision and Pattern
    Recognition,* 2017.

***IMPORTANT LINKS TO THEIR DATASET AND GITHUB REPOSITORY:***

1.  <http://kaldir.vc.in.tum.de/faceforensics_benchmark/>

2.  <https://github.com/ondyari/FaceForensics>

3.  <https://arxiv.org/abs/1901.08971>


