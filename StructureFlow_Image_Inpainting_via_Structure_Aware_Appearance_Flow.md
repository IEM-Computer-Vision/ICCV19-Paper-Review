# StructureFlow: Image Inpainting via Structure-aware Appearance Flow
[Somodyuti Pal](https://www.linkedin.com/in/somodyuti-pal-a01028136/)

 With development of deep neural networks, reconstructing of hazy,distorted images or removing unwanted objects is a demanding application. Termed as [Image Inpainting](https://en.wikipedia.org/wiki/Inpainting) this was approached by diffusion based,patch based and deep-neural network framework based methods
 but failed to generate realistic structures of missing areas and gave disappointing results on non-repetitive patterns as it lacked in separating **_structure_** and **_texture_** of an image.

 This Paper takes care of that by introducing two stages: 1.Structure reconstruction & 2. Texture generation. this two stages first recovers the missing structures and generate the reconstructed fine tuned data in last stage. Avoiding high frequency textures first edge images are used for structural guidance. This two stage network flow use Edge-preserved smooth methods to remove high-frequency textures while retaining sharp edges
 and low-frequency structures. 

<div align="center"><div>Structure Flow Architecture</div>
<img src="./images/Screenshot (347).png">
</div>
 
 ## The Proposed Method ##

 - Edge preserved smooth images are used to reconstruct missing sections.the use of appearance flow ensures sample features from regions with similar structures.
  Then texture generator synthesizes high frequency details. And
 Gaussian sampling is used instead of bilinear sampling to expand the receptive field of sampling operation.The model consists of two parts:
the **structure reconstructor**  <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ G_s &space; "> and the **Texture generator** <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ G_t &space; ">
   - **Structure Reconstructor** -  Let **I<sub>gt</sub>**  be the ground-truth image and **S<sub>gt</sub>**  be the
   edge-preserved smooth result of **I<sub>gt</sub>** .Now processing of the structure reconstructor will be 
   <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?S&space;=G_s&space;\(I_g_t,&space;S_i_n,&space;M\)"> where
   M is the mask of the input image I<sub>in</sub>. , a binarized    matrix where 1 represents the missing region   and 0 represents the background. 

       **The reconstruction loss** of <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ G_s &space; "> is defined as the <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\l_1 "> distance between the predicted structures 
       <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\widehat{S} "> and the groundtruth structures <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ S_{gt} ">. 
    
     To achieve <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ S_{gt} ">. GAN is applied and the ***Adversial loss*** is 
     
     
     <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?L_{adv}^s&space;=&space;-E [log(1-D_s(G_s(I_{in} , S_{in} ,M))] + E [log(D_s(S_{gt}))]  ">   

     where <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ D_s "> is discriminator.Now the generator & discriminator is trained & optimized as

     <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?min_{G_s} max_{D_s} L^s(G_s ,D_s )&space;=&space; \lambda^s_{l_1}L^s_{l_1}+ \lambda^s_{adv}L^s_{adv}">

     where <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ &space;\lambda^s_{l_1} &space; \lambda^s_{adv} "> are regularized parameters.


    - **Texture Generator** -  The texture generator takes <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\widehat{S} "> and carrys on to generated vivid textures.this is

      <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\widehat{I}= &space; G_t(I_{in} ,\widehat{S} ,M)"> .
      
      
       The adversial loss  in texture generator will be 
     <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?L_{adv}^s&space;=&space;-E [log(1-D_t(G_t(I_{in} , S_{in} ,M))] +  E [log(D_t (S_{gt}))]">  now to establish long term dependency & a proper relation between different reegions the apearence Flow is introduced.Gaussian sampling is proposed to expand the receptive field.with a kernel size n this can be written as 
     <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ F_o = \sum^n_{i=1} \sum^n_{j=1} \frac{a_{i,j}}{\sum^n_{i=1} \sum^n_{j=1}a_{i,j}}F_{i,j}">
       
       - **Sampling Correctness Loss** - This sampling correctnes loss playes a important key as it constraints the appearence flow .<img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ V_{in} "> and  <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ V_{gt} "> be features generated by VGG19 layer.The sampling correctness loss calculate the relative
       cosine similarity between the ground truth features and the sampled features as:

         <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ L_c^t = \frac{1}{N} \sum_{x,y \in M } exp(-\frac{\mu (V_{x,y}^{gt},V_{x+\triangle x ,y+\triangle y}^{in})}{\mu_{x,y}^{max}})">



         <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ V_{x+\triangle x ,y+\triangle y}^{in} &space;">  is the sampled feature calculated byGaussian sampling and <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\mu_{x,y}^{max}"> is a normalization term.


### **Experimental Results** -
  
  - Several resudial blocks with autoencoders were used to process the features.This model was tested **Places2**, **CelebA**,and **Paris StreetView** datasets. the structure reconstructor <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ G_ s ">. and the texture generator <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ G_ t ">. are trained separately using the edge-preserved image <img class="eqn-inline" src="https://latex.codecogs.com/svg.latex?\ S_ {gt} ">. 
 
 <div align="center"><div>Qualitative analysis</div>
 <img src="./images/Screenshot (351).png">
 </div>
  
  - This model was compared with CA,Edge Connect and it outperformed both of them.the edge-preserved smooth images
  had the structures of images retrived well so,the
  model well balanced structure reconstruction and texture
  generation. Photo-realistic results are obtained hence. As these remove high frequency textures the proper tradeoff between structure reconstructor and the texture
  generator was acheived .
 
 <div align="center"><div>Relative performance</div>
 <img src="./images/Screenshot (348).png">
 </div>
 
  - **Flow Ablation Study** revealed Gaussian sampling & sampling correction loss effected real good as Bilinear sampling failed to capture long term correlations.

 <div align="center"><div>PSNR ,SSIM & SID compare</div>
 <img src="./images/Screenshot (349).png">
 </div>


### **Conclusion** -  
Structure preservation and texture generator stages handled the inpaintaing challenge well.introducing appearance flow to to sample features from relative regions yield realistic image details,which improved the outcome in a geat manner. Experiments on multiple datasets shows the superior performance of the proposed network.

For code, visit this [link](https://github.com/RenYurui/StructureFlow).
       